{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "GWTC1.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ColmTalbot/gwpopulation/blob/master/examples/GWTC1.ipynb)"
   ]
  },
  {
   "metadata": {
    "id": "kiawJMuzaMqA",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Population Inference on GWTC-1\n",
    "\n",
    "The first gravitational-wave transient catalog [\"GWTC-1\"](https://arxiv.org/abs/1811.12907) includes all compact binary coalescences observed during Advanced LIGO/Virgo's first and second observing runs.\n",
    "\n",
    "`GWPopulation` builds upon [bilby](git.ligo.org/lscsoft/bilby) ([arXiv:1811.02042](https://arxiv.org/abs/1811.02042)) to provide simple, modular, user-friendly, population inference.\n",
    "\n",
    "Currently implemented models include:\n",
    "\n",
    "- One and two component mass distributions in primary mass and mass ratio, e.g., Talbot & Thrane (2018) ([arXiv:1801:02699](https://arxiv.org/abs/1801.02699)), Fishbach & Holz (2018) ([arXiv:1709.08584](https://arxiv.org/abs/1709.08584)).\n",
    "- The same mass distributions but independent but identically distributed primary and secondary.\n",
    "- Half-Gaussian + isotropic spin tilt distribution from Talbot & Thrane (2017) ([arXiv:1704.08370](https://arxiv.org/abs/1704.08370)).\n",
    "- Beta spin magnitude distribution from Wysocki+ (2018) ([arXiv:1805:06442](https://arxiv.org/abs/1805.06442)).\n",
    "- Each of these are also available with independent but identically distributed spins.\n",
    "- Redshift evolution model as in Fishbach+ (2018) ([arXiv:1805.10270](https://arxiv.org/abs/1805.10270)).\n",
    "- More to come and any contributions welcome...\n",
    "\n",
    "For more information see the [git repository](https://github.com/ColmTalbot/gwpopulation), [documentation](https://colmtalbot.github.io/gwpopulation/).\n"
   ]
  },
  {
   "metadata": {
    "id": "U6RH_xfNbBb3",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Install some packages\n",
    "\n",
    "- `gwpopulation` has the population model code.\n",
    "- `cupy` allows use to leverage the GPU.\n",
    "\n",
    "If you're using colab.research.google.com you will want to choose a GPU-accelerated runtime.\n",
    "\n",
    "\"runtime\"->\"change runtime type\"->\"Hardware accelerator = GPU\""
   ]
  },
  {
   "metadata": {
    "id": "WT13XqcsZoie",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "!pip install gwpopulation"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "1tjorhzLaoU2",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "!pip install cupy"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "rxjmzikYa0bb",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Get the data\n",
    "\n",
    "Pull the posterior samples for each of the events from the LIGO dcc."
   ]
  },
  {
   "metadata": {
    "id": "GXZpNd3cZ3hF",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "!wget https://dcc.ligo.org/public/0157/P1800370/002/GWTC-1_sample_release.tar.gz\n",
    "!tar -xvzf GWTC-1_sample_release.tar.gz"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "NLEgW_zrbNPw",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Imports\n",
    "\n",
    "Import the packages required for the script."
   ]
  },
  {
   "metadata": {
    "id": "7zJiHR7rayRR",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "%pylab inline\n",
    "\n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from astropy import cosmology, units\n",
    "\n",
    "import bilby as bb\n",
    "from bilby.core.prior import LogUniform, PriorDict, Uniform\n",
    "from bilby.hyper.model import Model\n",
    "import gwpopulation as gwpop\n",
    "xp = gwpop.cupy_utils.xp"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "hm3_uaQRbXmS",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Load posteriors\n",
    "\n",
    "We're using the posteriors from the GWTC-1 data release.\n",
    "\n",
    "We need to change the names of the parameters to make them work with the code."
   ]
  },
  {
   "metadata": {
    "id": "BojmLvpxbYwM",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "parameter_translator = dict(\n",
    "    mass_1_det='m1_detector_frame_Msun',\n",
    "    mass_2_det='m2_detector_frame_Msun',\n",
    "    luminosity_distance='luminosity_distance_Mpc',\n",
    "    a_1='spin1',\n",
    "    a_2='spin2',\n",
    "    cos_tilt_1='costilt1',\n",
    "    cos_tilt_2='costilt2')\n",
    "\n",
    "posteriors = list()\n",
    "priors = list()\n",
    "\n",
    "file_str = './GWTC-1_sample_release/GW{}_GWTC-1.hdf5'\n",
    "\n",
    "events = ['150914', '151012', '151226', '170104', '170608',\n",
    "          '170729', '170809', '170814', '170818', '170823']\n",
    "for event in events:\n",
    "    _posterior = pd.DataFrame()\n",
    "    _prior = pd.DataFrame()\n",
    "    with h5py.File(file_str.format(event)) as ff:\n",
    "        for my_key, gwtc_key in parameter_translator.items():\n",
    "            _posterior[my_key] = ff['IMRPhenomPv2_posterior'][gwtc_key]\n",
    "            _prior[my_key] = ff['prior'][gwtc_key]\n",
    "    posteriors.append(_posterior)\n",
    "    priors.append(_prior)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "ol2czCPkblx6",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "luminosity_distances = np.linspace(1, 10000, 1000)\n",
    "redshifts = np.array([\n",
    "    cosmology.z_at_value(\n",
    "        cosmology.Planck15.luminosity_distance, dl * units.Mpc)\n",
    "    for dl in luminosity_distances])\n",
    "dl_to_z = interp1d(luminosity_distances, redshifts)\n",
    "\n",
    "luminosity_prior = luminosity_distances ** 2\n",
    "\n",
    "dz_ddl = np.gradient(redshifts, luminosity_distances)\n",
    "\n",
    "redshift_prior = interp1d(\n",
    "    redshifts, luminosity_prior / dz_ddl / (1 + redshifts))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "9zlAGxTxRUIn",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Add some weights to posterior\n",
    "\n",
    "Make sure the posterior `DataFrames` contain the appropriate quantities.\n",
    "\n",
    "We could include a `prior` column, this is the prior used in the initial sampling stage.\n",
    "This is used to weight the samples in the likelihood."
   ]
  },
  {
   "metadata": {
    "id": "hTqr-NvTbn4c",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "for posterior in posteriors:\n",
    "    posterior['redshift'] = dl_to_z(posterior['luminosity_distance'])\n",
    "    posterior['mass_1'] = posterior['mass_1_det'] / (1 + posterior['redshift'])\n",
    "    posterior['mass_2'] = posterior['mass_2_det'] / (1 + posterior['redshift'])\n",
    "    posterior['mass_ratio'] = posterior['mass_2'] / posterior['mass_1']"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "Bx1xdvjubhmA",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Specify the model\n",
    "\n",
    "Choose which population models we want to use. \n",
    "\n",
    "For the mass distribution we use \n",
    "\n",
    "`gwpopulation.models.mass.two_component_primary_mass_ratio`.\n",
    "\n",
    "This is a powerlaw + Gaussian mass distribution with powerlaw mass ratio distribution.\n",
    "\n",
    "For spins we use\n",
    "\n",
    "`gwpopulation.models.spin.iid_spin`\n",
    "\n",
    "Where the spins of the two black holes are independently and identically distirbuted with a beta distribution for the magnitude and an isotropic + half-Gaussian for the cosine tilts."
   ]
  },
  {
   "metadata": {
    "id": "rHspMjv-bpyZ",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "model = Model([gwpop.models.mass.two_component_primary_mass_ratio])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "7NwQL52xbrxr",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Selection effects\n",
    "\n",
    "Gravitational-wave surveys suffer from Malmquist bias.\n",
    "\n",
    "In order to measure the true, astrophysical, distribution we must include a term to account for this in our population analyses.\n",
    "\n",
    "The way the likelihood is structured, this can be any object that evaluates to give the observed spactime volume as a function of the population parameters.\n",
    "\n",
    "We define classes so that various bits of metadata can be stored.\n",
    "\n",
    "The data for calculating this is not easily available.\n",
    "We use a very rough toy model to get the general scaling for the primary mass, $VT(m) \\sim m^{1.6}$.\n",
    "This value was chosen to get a decent agreement with the more complex model.\n",
    "\n",
    "**I do not recommend using this toy function for science.**"
   ]
  },
  {
   "metadata": {
    "id": "wmgJOa57bttM",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "masses = xp.linspace(3, 100, 1000)\n",
    "vts = masses**1.6\n",
    "\n",
    "def toy_vt_calculator(kwargs):\n",
    "    params = {key: kwargs[key] for key in \n",
    "              ['alpha', 'mmin', 'mmax', 'lam', 'mpp', 'sigpp']}\n",
    "    p_m = gwpop.models.mass.two_component_single(\n",
    "        masses, **params)\n",
    "    return gwpop.cupy_utils.trapz(p_m * vts, masses)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "LsHr0RCCb18B",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Define the likelihood\n",
    "\n",
    "The `HyperparameterLikelihood` marginalises over the local merger rate, with a uniform-in-log prior.\n",
    "\n",
    "To also estimate the rate use the `RateLikelilhood` (see further on in the notebook).\n",
    "\n",
    "We provide:\n",
    "- `posteriors`: a list of `pandas` DataFrames\n",
    "- `hyper_prior`: our population model, as defined above\n",
    "- `selection_function`: anything which evaluates the selection function\n",
    "\n",
    "We can also provide:\n",
    "- `conversion_function`: this converts between the parameters we sample in and those needed by the model, e.g., for sampling in the mean and variance of the beta distribution\n",
    "- `max_samples`: the maximum number of samples to use from each posterior, this defaults to the length of the shortest posterior\n",
    "\n",
    "We may get a warning telling us `cupy` is not available and so `numpy` is for the likelihood evaluation.\n",
    "This will go away if you have a GPU and `cupy` installed."
   ]
  },
  {
   "metadata": {
    "id": "0NhriPjTbzT7",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "fast_likelihood = gwpop.hyperpe.HyperparameterLikelihood(\n",
    "    posteriors=posteriors, hyper_prior=model,\n",
    "    selection_function=toy_vt_calculator)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "6HuGzAh1b7FQ",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Define the prior\n",
    "\n",
    "This is the standard method to define the prior distribution within `bilby`.\n",
    "\n",
    "The labels are used in plotting.\n",
    "\n",
    "Numbers are converted to delta function priors and are not sampled.\n",
    "\n",
    "There are many other distributions available, see the code/documentation for a full list."
   ]
  },
  {
   "metadata": {
    "id": "7pD90QeEb9aV",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "fast_priors = PriorDict()\n",
    "\n",
    "# mass\n",
    "fast_priors['alpha'] = Uniform(minimum=-2, maximum=4, latex_label='$\\\\alpha$')\n",
    "fast_priors['beta'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\beta$')\n",
    "fast_priors['mmin'] = Uniform(minimum=5, maximum=10, latex_label='$m_{\\\\min}$')\n",
    "fast_priors['mmax'] = Uniform(minimum=20, maximum=60, latex_label='$m_{\\\\max}$')\n",
    "fast_priors['lam'] = Uniform(minimum=0, maximum=1, latex_label='$\\\\lambda_{m}$')\n",
    "fast_priors['mpp'] = Uniform(minimum=10, maximum=50, latex_label='$\\\\mu_{m}$')\n",
    "fast_priors['sigpp'] = Uniform(minimum=0, maximum=10, latex_label='$\\\\sigma_{m}$')\n",
    "# spin\n",
    "fast_priors['amax'] = 1\n",
    "fast_priors['alpha_chi'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\alpha_{\\\\chi}$')\n",
    "fast_priors['beta_chi'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\beta_{\\\\chi}$')\n",
    "fast_priors['xi_spin'] = Uniform(minimum=0, maximum=1, latex_label='$\\\\xi$')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "hOXy6gHmcAVf",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Run the sampler\n",
    "\n",
    "We'll use the sampler `dynesty` and use a small number of live points to reduce the runtime.\n",
    "\n",
    "This is painfully slow without using the GPU version.\n",
    "If you have a GPU it will just work.\n",
    "\n",
    "Other samplers are available, `cpnest` gave the best results for the O1+O2 data, however it isn't currently compatible with the GPU likelihood.\n",
    "\n",
    "`bilby` times a single likelihood evaluation before beginning the run\n",
    "\n",
    "We do a call before running to sampler as `cupy` compiles kernels the first time they are evaluated and so the estimate of the evaluation time would be off."
   ]
  },
  {
   "metadata": {
    "id": "EFGmgznvcC4s",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "fast_likelihood.parameters.update(fast_priors.sample())\n",
    "fast_likelihood.log_likelihood_ratio()\n",
    "\n",
    "fast_result = bb.run_sampler(\n",
    "    likelihood=fast_likelihood, priors=fast_priors, sampler='dynesty',\n",
    "    nlive=100, label='fast')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "vSrM3Dy1zsKL",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "fast_result.plot_corner(save=False)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "hBfS17v47-zu",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Define a new model\n",
    "\n",
    "### Let's define a new population model for BNS. \n",
    "\n",
    "Just as an example we'll use a Gaussian distribution bounded between $[1 M_{\\odot}, 2 M_{\\odot}]$.\n",
    "\n",
    "$$p(m_1, m_2) = N \\exp \\left(- \\frac{\\left((m_1 - \\mu)^2 + (m_2 - \\mu)^2\\right)}{2 \\sigma^2}\\right) \\quad : \\quad 1 \\leq m_2 \\leq m_1 \\leq 2$$\n",
    "\n",
    "We see that this function takes three arguments:\n",
    "- `dataset`: this is common to all of the population models in `gwpopulation`, it is a dictionary containing the data to be evaluated, here it is assumed to contain entries for `mass_1` and `mass_2`, the _source-frame_ masses.\n",
    "- `mu_bns`: the peak of the bns mass distribution.\n",
    "- `sigma_bns`: the width of the bns mass distribution."
   ]
  },
  {
   "metadata": {
    "id": "qRKha-Mv8Zav",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "def truncated_gaussian_primary_secondary_identical(dataset, mu_bns, sigma_bns):\n",
    "    prob = gwpop.utils.truncnorm(\n",
    "        dataset['mass_1'], mu=mu_bns, sigma=sigma_bns, low=1, high=2)\n",
    "    prob *= gwpop.utils.truncnorm(\n",
    "        dataset['mass_2'], mu=mu_bns, sigma=sigma_bns, low=1, high=2)\n",
    "    prob *= (dataset['mass_1'] >= dataset['mass_2'])\n",
    "    prob *= 2\n",
    "    return prob"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "IflaTL8l9TJQ",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Load GW170817 posterior\n",
    "\n",
    "This is just the same as above."
   ]
  },
  {
   "metadata": {
    "id": "ubZz578B9ATE",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "posterior = pd.DataFrame()\n",
    "prior = pd.DataFrame()\n",
    "with h5py.File('./GWTC-1_sample_release/GW170817_GWTC-1.hdf5') as ff:\n",
    "    for my_key, gwtc_key in parameter_translator.items():\n",
    "        try:\n",
    "            posterior[my_key] = ff['IMRPhenomPv2NRT_lowSpin_posterior'][gwtc_key]\n",
    "            prior[my_key] = ff['IMRPhenomPv2NRT_lowSpin_prior'][gwtc_key]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "posterior['redshift'] = dl_to_z(posterior['luminosity_distance'])\n",
    "posterior['mass_1'] = posterior['mass_1_det'] / (1 + posterior['redshift'])\n",
    "posterior['mass_2'] = posterior['mass_2_det'] / (1 + posterior['redshift'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "ktT2Ydiw9Yak",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Define the new likelihood\n",
    "\n",
    "We use the same likelihood as before.\n",
    "\n",
    "_Note_:\n",
    "- This time we cast our posterior to a list while creating the likelihood.\n",
    "- We pass the function rather than a `Model` object as before, `bilby` will turn this into a `Model` for internal use.\n",
    "- We've removed the selection and conversion functions as they aren't needed here (yes, a selection function is techinically needed)."
   ]
  },
  {
   "metadata": {
    "id": "0xqYdOKV9F1E",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "bns_likelihood = gwpop.hyperpe.HyperparameterLikelihood(\n",
    "    posteriors=[posterior],\n",
    "    hyper_prior=truncated_gaussian_primary_secondary_identical)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "iGHtErVA9hBb",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Define the new prior\n",
    "\n",
    "Just as before."
   ]
  },
  {
   "metadata": {
    "id": "gn_vPWIW9MED",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "bns_priors = PriorDict()\n",
    "bns_priors['mu_bns'] = Uniform(minimum=1, maximum=2, latex_label='$\\\\mu_{bns}$')\n",
    "bns_priors['sigma_bns'] = LogUniform(minimum=1e-2, maximum=1, latex_label='$\\\\sigma_{bns}$')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "d_gwpGQi9e0M",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "bns_likelihood.parameters.update(bns_priors.sample())\n",
    "bns_likelihood.log_likelihood_ratio()\n",
    "\n",
    "bns_result = bb.run_sampler(\n",
    "    likelihood=bns_likelihood, priors=bns_priors, sampler='dynesty',\n",
    "    nlive=1000)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "Y1O73fzb9t1S",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "bns_result.plot_corner(save=False)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "mxXt8coXANxX",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Do it all\n",
    "\n",
    "Let's put together a run with models for the mass, spin and redshift distributions.\n",
    "\n",
    "**This will not give sensible answers because VT is not estimated.**\n",
    "\n",
    "The data for VT estimation isn't available in this notebook.\n",
    "\n",
    "Note that the redshift model is a class and so is called slightly differently.\n",
    "This is to enable caching of expensive data internally.\n",
    "To call this `bilby>=0.4.2` is required."
   ]
  },
  {
   "metadata": {
    "id": "LTq6eP5qAgLm",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "full_model = Model([\n",
    "    gwpop.models.mass.two_component_primary_mass_ratio,\n",
    "    gwpop.models.spin.iid_spin_magnitude_beta,\n",
    "    gwpop.models.spin.independent_spin_orientation_gaussian_isotropic,\n",
    "    gwpop.models.redshift.PowerLawRedshift()])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "T4Ize_dKRtC_",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Update sampling prior\n",
    "\n",
    "We need to update the sampling prior to account for the new redshift evolution model.\n",
    "\n",
    "Fortunately, we defined an interpolant for this earlier."
   ]
  },
  {
   "metadata": {
    "id": "7i0WSBDZRunm",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "for posterior in posteriors:\n",
    "    posterior['prior'] = redshift_prior(posterior['redshift'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "5JxwG1xNSJ_9",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Likelihood\n",
    "\n",
    "We use a different likelihood class `RateLikelihood`, this will simultaneously estimate the local merger rate as well as the population distribution.\n",
    "\n",
    "This is created just as before."
   ]
  },
  {
   "metadata": {
    "id": "v-o0X0OiBaug",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "full_likelihood = gwpop.hyperpe.RateLikelihood(\n",
    "    posteriors=posteriors, hyper_prior=full_model)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "RkzUAuX4SNQ_",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "## Prior\n",
    "\n",
    "This is just a longer version of the previous."
   ]
  },
  {
   "metadata": {
    "id": "F2DXzB_KA8sR",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "full_priors = PriorDict()\n",
    "\n",
    "# rate\n",
    "fast_priors['rate'] = LogUniform(minimum=1e-20, maximum=1e20, latex_label='$R$')\n",
    "# mass\n",
    "full_priors['alpha'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\alpha$')\n",
    "full_priors['beta'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\beta$')\n",
    "full_priors['mmin'] = Uniform(minimum=5, maximum=10, latex_label='$m_{\\\\min}$')\n",
    "full_priors['mmax'] = Uniform(minimum=20, maximum=60, latex_label='$m_{\\\\max}$')\n",
    "full_priors['lam'] = Uniform(minimum=0, maximum=1, latex_label='$\\\\lambda_{m}$')\n",
    "full_priors['mpp'] = Uniform(minimum=20, maximum=50, latex_label='$\\\\mu_{m}$')\n",
    "full_priors['sigpp'] = Uniform(minimum=0, maximum=10, latex_label='$\\\\sigma_{m}$')\n",
    "# spin magnitude\n",
    "full_priors['amax'] = 1\n",
    "full_priors['alpha_chi'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\alpha_{\\\\chi}$')\n",
    "full_priors['beta_chi'] = Uniform(minimum=-4, maximum=12, latex_label='$\\\\beta_{\\\\chi}$')\n",
    "# spin orientation\n",
    "full_priors['xi_spin'] = Uniform(minimum=0, maximum=1, latex_label='$\\\\xi$')\n",
    "full_priors['sigma_1'] = Uniform(minimum=0, maximum=4, latex_label='$\\\\sigma{1}$')\n",
    "full_priors['sigma_2'] = Uniform(minimum=0, maximum=4, latex_label='$\\\\sigma{2}$')\n",
    "# redshift evolution\n",
    "full_priors['lamb'] = Uniform(minimum=-25, maximum=25, latex_label='$\\\\lambda_{z}$')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "metadata": {
    "id": "PDd-8Z_WBXEs",
    "colab_type": "code",
    "colab": {}
   },
   "cell_type": "code",
   "source": [
    "full_likelihood.parameters.update(full_priors.sample())\n",
    "full_likelihood.log_likelihood_ratio()\n",
    "\n",
    "full_result = run_sampler(\n",
    "    likelihood=full_likelihood, priors=full_priors, sampler='dynesty',\n",
    "    nlive=100)"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}